{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.f1_score_calculator import calculate_f1_score\n",
    "import re\n",
    "from utils.extract_raw import quant_match\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating F1 Scores on raw preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_epochs = pd.read_csv('../../results_csv/2_epochs_16k.csv')\n",
    "three_epochs = pd.read_csv('../../results_csv/3_epochs_16k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0  entity_name                                  kaggle_image_path  \\\n",
       " 0           0  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/514...   \n",
       " 1           1  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/81k...   \n",
       " 2           2  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/51T...   \n",
       " 3           3        depth  /kaggle/input/amazon-2024-100k-imgs/images/51r...   \n",
       " 4           4       height  /kaggle/input/amazon-2024-100k-imgs/images/51N...   \n",
       " \n",
       "       entity_value       prediction  \n",
       " 0        20.0 gram          20 gram  \n",
       " 1         5.2 gram  300.0 milligram  \n",
       " 2       328.0 gram       128.0 gram  \n",
       " 3        1.26 inch  6.31 millimetre  \n",
       " 4  40.0 centimetre  40.0 centimetre  ,\n",
       "    Unnamed: 0  entity_name                                  kaggle_image_path  \\\n",
       " 0           0  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/514...   \n",
       " 1           1  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/81k...   \n",
       " 2           2  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/51T...   \n",
       " 3           3        depth  /kaggle/input/amazon-2024-100k-imgs/images/51r...   \n",
       " 4           4       height  /kaggle/input/amazon-2024-100k-imgs/images/51N...   \n",
       " \n",
       "       entity_value       prediction  \n",
       " 0        20.0 gram        20.0 gram  \n",
       " 1         5.2 gram  300.0 milligram  \n",
       " 2       328.0 gram       128.0 gram  \n",
       " 3        1.26 inch  5.31 millimetre  \n",
       " 4  40.0 centimetre  40.0 centimetre  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_epochs.head(), three_epochs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating F1 Score: 100%|██████████| 20000/20000 [00:00<00:00, 1114202.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True Positives': 7775,\n",
       " 'False Positives': 12225,\n",
       " 'False Negatives': 0,\n",
       " 'Precision': 0.38875,\n",
       " 'Recall': 1.0,\n",
       " 'F1-Score': 0.5598559855985599}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_f1_score(two_epochs['entity_value'], two_epochs['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating F1 Score: 100%|██████████| 20000/20000 [00:00<00:00, 527724.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True Positives': 8601,\n",
       " 'False Positives': 11399,\n",
       " 'False Negatives': 0,\n",
       " 'Precision': 0.43005,\n",
       " 'Recall': 1.0,\n",
       " 'F1-Score': 0.6014475018356001}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_f1_score(three_epochs['entity_value'], three_epochs['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>entity_name</th>\n",
       "      <th>kaggle_image_path</th>\n",
       "      <th>entity_value</th>\n",
       "      <th>prediction</th>\n",
       "      <th>regex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>/kaggle/input/amazon-2024-100k-imgs/images/514...</td>\n",
       "      <td>20.0 gram</td>\n",
       "      <td>20 gram</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>/kaggle/input/amazon-2024-100k-imgs/images/81k...</td>\n",
       "      <td>5.2 gram</td>\n",
       "      <td>300.0 milligram</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>item_weight</td>\n",
       "      <td>/kaggle/input/amazon-2024-100k-imgs/images/51T...</td>\n",
       "      <td>328.0 gram</td>\n",
       "      <td>128.0 gram</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>depth</td>\n",
       "      <td>/kaggle/input/amazon-2024-100k-imgs/images/51r...</td>\n",
       "      <td>1.26 inch</td>\n",
       "      <td>6.31 millimetre</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>height</td>\n",
       "      <td>/kaggle/input/amazon-2024-100k-imgs/images/51N...</td>\n",
       "      <td>40.0 centimetre</td>\n",
       "      <td>40.0 centimetre</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  entity_name                                  kaggle_image_path  \\\n",
       "0           0  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/514...   \n",
       "1           1  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/81k...   \n",
       "2           2  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/51T...   \n",
       "3           3        depth  /kaggle/input/amazon-2024-100k-imgs/images/51r...   \n",
       "4           4       height  /kaggle/input/amazon-2024-100k-imgs/images/51N...   \n",
       "\n",
       "      entity_value       prediction regex  \n",
       "0        20.0 gram          20 gram        \n",
       "1         5.2 gram  300.0 milligram        \n",
       "2       328.0 gram       128.0 gram        \n",
       "3        1.26 inch  6.31 millimetre        \n",
       "4  40.0 centimetre  40.0 centimetre        "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_epochs['regex'] = ''\n",
    "two_epochs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:05, 3967.96it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, value in tqdm(enumerate(two_epochs['prediction'])):\n",
    "    val = quant_match(value)\n",
    "    two_epochs.loc[idx, 'regex'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:04, 4444.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, value in tqdm(enumerate(three_epochs['prediction'])):\n",
    "    val = quant_match(value)\n",
    "    three_epochs.loc[idx, 'regex'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0  entity_name                                  kaggle_image_path  \\\n",
       " 0           0  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/514...   \n",
       " 1           1  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/81k...   \n",
       " 2           2  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/51T...   \n",
       " 3           3        depth  /kaggle/input/amazon-2024-100k-imgs/images/51r...   \n",
       " 4           4       height  /kaggle/input/amazon-2024-100k-imgs/images/51N...   \n",
       " \n",
       "       entity_value       prediction            regex  \n",
       " 0        20.0 gram          20 gram          20 gram  \n",
       " 1         5.2 gram  300.0 milligram  300.0 milligram  \n",
       " 2       328.0 gram       128.0 gram       128.0 gram  \n",
       " 3        1.26 inch  6.31 millimetre  6.31 millimetre  \n",
       " 4  40.0 centimetre  40.0 centimetre  40.0 centimetre  ,\n",
       "    Unnamed: 0  entity_name                                  kaggle_image_path  \\\n",
       " 0           0  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/514...   \n",
       " 1           1  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/81k...   \n",
       " 2           2  item_weight  /kaggle/input/amazon-2024-100k-imgs/images/51T...   \n",
       " 3           3        depth  /kaggle/input/amazon-2024-100k-imgs/images/51r...   \n",
       " 4           4       height  /kaggle/input/amazon-2024-100k-imgs/images/51N...   \n",
       " \n",
       "       entity_value       prediction            regex  \n",
       " 0        20.0 gram        20.0 gram        20.0 gram  \n",
       " 1         5.2 gram  300.0 milligram  300.0 milligram  \n",
       " 2       328.0 gram       128.0 gram       128.0 gram  \n",
       " 3        1.26 inch  5.31 millimetre  5.31 millimetre  \n",
       " 4  40.0 centimetre  40.0 centimetre  40.0 centimetre  )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_epochs.head(), three_epochs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating F1 Score: 100%|██████████| 20000/20000 [00:00<00:00, 589812.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True Positives': 7775,\n",
       " 'False Positives': 12225,\n",
       " 'False Negatives': 0,\n",
       " 'Precision': 0.38875,\n",
       " 'Recall': 1.0,\n",
       " 'F1-Score': 0.5598559855985599}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_f1_score(two_epochs['entity_value'], two_epochs['regex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating F1 Score: 100%|██████████| 20000/20000 [00:00<00:00, 542011.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'True Positives': 8601,\n",
       " 'False Positives': 11399,\n",
       " 'False Negatives': 0,\n",
       " 'Precision': 0.43005,\n",
       " 'Recall': 1.0,\n",
       " 'F1-Score': 0.6014475018356001}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_f1_score(three_epochs['entity_value'], three_epochs['regex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 had 0.6 F1-score!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
